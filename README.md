# Deep-Learning-Certification


This repositoriy contains 3 folders #Week2, #Week3 and #Week4, both containing ipython notebook files.

From the first folder (Week2), you'll learn how to
- Build the general architecture of a learning algorithm, with
	* Initializing parameters
	* Calculating the cost function and its gradient
	* Using an optimization algorithm (gradient descent).

- Gather all three functions above into a main model function, in the right order.

From the second folder (Week3), you'll understand the difference between this model and the previous one in term of precision. So you'll learn how to:
- Implement a 2-class classification neural network with a single hidden-layer
- Use units with non-linear activation function, such as tanh
- Compute the cross entropy loss
- Implement forward and backward propagation

The last folder contains two ipynb files. For best comprehension, follow them in this order:
- The first 'Building+your+Deep+Neural+Network+-+Step+by+Step+v8.ipynb' will help you understand how to
	* Use non-linear units like ReLU to improve your model.
	* Build a deeper neural network (with more than 1 hidden-layer)
	* Implement an easy-to-use neural network class
	
- In the second one, you'll be able to build and apply a deep neural network to supervised learning

